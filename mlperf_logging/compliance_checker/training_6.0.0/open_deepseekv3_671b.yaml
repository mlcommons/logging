- KEY:
    NAME:  global_batch_size
    REQ:   EXACTLY_ONE
    POST: >
        s['global_batch_size'] = v['value']

- KEY:
    NAME:  max_sequence_length
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 4096 "

- KEY:
    NAME:  opt_name
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 'adamw' "

- KEY:
    NAME:  max_steps
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 12000 "
    POST: >
        s['max_steps'] = v['value']

- KEY:
    NAME:  opt_base_learning_rate
    REQ:   EXACTLY_ONE
    CHECK: " abs(v['value'] - 2.4e-05 * (s['global_batch_size'] / 16384) ** 0.5) < 1e-9 "

- KEY:
    NAME:  opt_learning_rate_warmup_steps
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 4 "
    POST: >
        s['opt_learning_rate_warmup_steps'] = v['value']

- KEY:
    NAME:  opt_learning_rate_decay_steps
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == s['max_steps'] - s['opt_learning_rate_warmup_steps'] "

- KEY:
    NAME:  opt_learning_rate_decay_schedule
    REQ:   EXACTLY_ONE

- KEY:
    NAME:  opt_adamw_beta_1
    REQ:   EXACTLY_ONE

- KEY:
    NAME:  opt_adamw_beta_2
    REQ:   EXACTLY_ONE

- KEY:
    NAME:  opt_adamw_epsilon
    REQ:   EXACTLY_ONE

- KEY:
    NAME:  opt_adamw_weight_decay
    REQ:   EXACTLY_ONE

- KEY:
    NAME:  opt_gradient_clip_norm
    REQ:   EXACTLY_ONE

- KEY:
    NAME:  gradient_accumulation_steps
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] > 0 "

- KEY:
    NAME:  eval_samples
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 1024 "

- KEY:
    NAME:  eval_accuracy
    REQ:   AT_LEAST_ONE
    CHECK:
        - "'epoch_num' in v['metadata']"
    ATLEAST_ONE_CHECK: "(v['value'] <= 4.05) and v['value'] > 0.0"

