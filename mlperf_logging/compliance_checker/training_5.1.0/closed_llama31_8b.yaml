- KEY:
    NAME:  global_batch_size
    REQ:   EXACTLY_ONE
    POST: >
        s['global_batch_size'] = v['value']

- KEY:
    NAME:  max_sequence_length
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 8192 "

- KEY:
    NAME:  opt_name
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 'adamw' "

- KEY:
    NAME:  opt_base_learning_rate
    REQ:   EXACTLY_ONE

- KEY:
    NAME:  opt_end_learning_rate
    REQ:   EXACTLY_ONE

- KEY:
    NAME:  opt_learning_rate_warmup_steps
    REQ:   EXACTLY_ONE
    POST: >
        s['opt_learning_rate_warmup_steps'] = v['value']

- KEY:
    NAME:  opt_learning_rate_decay_steps
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == math.ceil(1_200_000 / s['global_batch_size'] ) - s['opt_learning_rate_warmup_steps'] "

- KEY:
    NAME:  opt_learning_rate_decay_schedule
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 'cosine with linear warmup' "

- KEY:
    NAME:  opt_adamw_beta_1
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 0.9 "

- KEY:
    NAME:  opt_adamw_beta_2
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 0.95 "

- KEY:
    NAME:  opt_adamw_epsilon
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 1e-05 "

- KEY:
    NAME:  opt_adamw_weight_decay
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 0.1 "

- KEY:
    NAME:  opt_gradient_clip_norm
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 1.0 "

- KEY:
    NAME:  gradient_accumulation_steps
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] > 0 "

- KEY:
    NAME:  eval_samples
    REQ:   EXACTLY_ONE
    CHECK: " v['value'] == 1024 "

- KEY:
    NAME:  eval_accuracy
    REQ:   AT_LEAST_ONE
    CHECK:
        - "'samples_count' in v['metadata']"
    ATLEAST_ONE_CHECK: "(v['value'] <= 3.3) and v['value'] > 0.0"

- KEY:
    NAME: max_steps
    REQ:  EXACTLY_ONE
    CHECK: " v['value'] == 1200000 "

